###Non-Deterministic Search
####Markov Decision Process
A Markov Decision Process is defined by several properties:
- a set of states S, a set of actions A
- a start, possibly one or more terminal states
- possibly a **discount factor $\gamma$**
- a **transition function $T(s, a, s^{\prime})$**: a probability function which
represents the probability that an agent taking an action $a \in A$ from state
$s \in S$ ends up in a state $s^{\prime} \in S$
&emsp; $\boxed {T(s, a, s^{\prime}) = P(s^{\prime} | s, a)}$
- a *reward function $R(s, a, s^{\prime})$*

#####Finite Horizons and Discounting
*Finite Horizons*: it essentially defines a "lifetime" for agents, which gives
them some set number of timesteps n to accrue as much reward as they can before
being automatically terminated.

*Discounting*:
Additive utility:$U([s_0, a_0, s_1, a_1, s_2,\dots]) = R(s_0, a_0, s_1) + R(s_1, a_1, s_2) +
R(s_2, a_2, s_3) + \dots$
Discounted utility:
$$
\begin{align}
U([s_0, a_0, s_1, a_1, s_2,\dots]) & = R(s_0, a_0, s_1) + \gamma R(s_1, a_1, s_2) +
\gamma ^ {2} R(s_2, a_2, s_3) + \dots\\
& = \sum \limits _{t=0} ^{\infty} \gamma ^{t} R(s_t, a_t, s_{t+1})\\
& \leq \sum \limits _{t = 0} ^{\infty} \gamma ^{t} R_{max}\\
& = \boxed{\frac{R_{max}}{1 - \gamma}}
\end{align}
$$

####The Bellman Equation
**$V^{*}(s)$**: the optimal value of a state
**$Q^{*}(s, a)$**: the optimal value of a q-state
$$V^{*} = \max \limits _{a} \sum \limits _{s^{\prime}} T(s, a, s^{\prime})
[R(s, a, s^{\prime}) + \gamma V^{*}(s^{\prime})]$$
$$Q^{*}(s, a) = \sum \limits _{s^{\prime}} T(s, a, s^{\prime})
[R(s, a, s^{\prime}) + \gamma V^{*}(s^{\prime})]$$

####Value Iteration
$V_{k}(s)$: the maximum expected utility attainable from $s$ given that the
Markov decision process under consideration terminates in $k$ timesteps
$V^{*}(s)$ for any terminal state must be 0, since no actions can ever be taken
from any terminal state to reap any rewards.
The operations are as follows:
1. $\forall s \in S$, initialize $V_0(s) = 0$
2. Repeat the following update rule until convergence
$$\boxed {\forall s \in S, V_{k+1}(s) \leftarrow \max \limits _{a} \sum \limits _{s ^{\prime}}
T(s, a, s^{\prime}) \left[R(s, a, s^{\prime}) + \gamma V_{k}(s^{\prime})\right]}$$

####Policy Iteration
Value iteration can be quite slow for two reasons:
- At each iteration, we must update the value of all states, each of which requires
iteration over all |A| actions. The runtime is $O(|S|^2||A|)$.
- Value iteration tends to do a lot of overcomputation since the policy as computed
by policy extraction generally converges significantly faster than the value themselves.

####Policy Extraction
$$\boxed {\forall s in S, \pi ^{*} (s) = \arg \max \limits _{a} Q ^{*} (s, a)
= \arg \max \limits _{a} \sum \limits _{s^{\prime}} T(s, a, s^{\prime})
[R(s, a, s^{\prime}) + \gamma V^{*}(s^{\prime})]}$$

Policy iteration operates as follows:
1. Define an initial policy
2. Repeat the following until converges
   - Evaluate the current policy
   $$\boxed {V^{\pi}(s) = \sum \limits _{s ^{\prime}} T(s, \pi (s), s^{\prime})
   [R(s, \pi(s), s^{\prime}) + \gamma V^{\pi}(s^{\prime})]}$$
   &emsp; Define the policy at iteration $i$ of policy iteration as $\pi_{i}$.
   Since we are fixing a single action for each state, we no longer need
   the max operator which effectively leaves us with a system of $|S|$
   equations generated by the above rule.
   $$V^{\pi_{i}}_{k+1}(s) \leftarrow \sum \limits _{s ^{\prime}}
   T(s, \pi_{i}(s), s^{\prime})[R(s, \pi_{i}(s), s^{\prime}) +
   \gamma V^{\pi_{i}}_{k}(s^{\prime})]$$
   - $\boxed {\pi _{i+1} (s) = \arg \max \limits _{a} \sum \limits _{s^{\prime}}
   T(s, a, s^{\prime})[R(s, a, s^{\prime}) + \gamma V^{\pi _{i}} (s^{\prime})]}$
